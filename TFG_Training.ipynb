{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Realizamos todas las importaciones, además de realizar algunas configuraciones previas, al ejecutarse en local debemos declarar correctamente donde se va a situar el archivo con los datos de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d8b6L0fN_GlD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import model\n",
        "import dataset\n",
        "from src.utils import set_logger, get_pck_with_sigma, get_pred_coordinates\n",
        "from src.loss import sum_mse_loss\n",
        "from easydict import EasyDict as edict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from PIL import Image\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Asignamos todos las rutas y carpetas necesesarias para el proyecto, los hiperparámetros y comprobamos que la GPU se encuentra disponible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "erMWpkge_NrD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "************** Experiment Name: EXP_Panoptic_ARB **************\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the log directory exists!\n",
            "Cuda ->  True\n"
          ]
        }
      ],
      "source": [
        "# ***********************  Parameter  ***********************\n",
        "\n",
        "args = edict({\n",
        "    \"config_file\": 'data_sample/Panoptic_base.json',\n",
        "    \"GPU\": 0\n",
        "})\n",
        "configs = json.load(open(args.config_file)) # type: ignore\n",
        "\n",
        "target_sigma_list = [0.05, 0.1, 0.15, 0.2, 0.25]\n",
        "select_sigma = 0.15\n",
        "\n",
        "model_name = 'EXP_' + configs[\"name\"]\n",
        "torch.cuda.empty_cache()\n",
        "save_dir = os.path.join(model_name, 'checkpoint/')\n",
        "test_pck_dir = os.path.join(model_name, 'test/')\n",
        "\n",
        "if os.path.exists(model_name):\n",
        "    print(\"the log directory exists!\")\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "os.makedirs(test_pck_dir, exist_ok=True)\n",
        "\n",
        "# training parameters ****************************\n",
        "data_root = configs[\"data_root\"]\n",
        "learning_rate = configs[\"learning_rate\"]\n",
        "batch_size = configs[\"batch_size\"]\n",
        "epochs = 100\n",
        "# data parameters ****************************\n",
        "\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"Cuda -> \",cuda)\n",
        "\n",
        "\n",
        "logger = set_logger(os.path.join(model_name, 'train.log'))\n",
        "logger.info(\"************** Experiment Name: {} **************\".format(model_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Construcción del modelo "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "x2-spsNG_j0k"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Create Model ...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the number of params: {'Total': 1945122, 'Trainable': 1945122}\n"
          ]
        }
      ],
      "source": [
        "# ******************** build model ********************\n",
        "logger.info(\"Create Model ...\")\n",
        "\n",
        "model = model.light_Model(configs)\n",
        "if cuda:\n",
        "    model = model.cuda()\n",
        "\n",
        "def get_parameter_number(model):\n",
        "    total_num = sum(p.numel() for p in model.parameters())\n",
        "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    # for name, p in model.named_parameters():\n",
        "    #     if p.requires_grad:\n",
        "    #         print(name, p.numel())\n",
        "    return {'Total': total_num, 'Trainable': trainable_num}\n",
        "print('the number of params:', get_parameter_number(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cargamos el Dataset sobre los dataloaders, preparando los 3 conjuntos de imágenes que vamos a usar(entrenamiento, validación y test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yjMhX0zH_piP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Total images in training data is 11853\n",
            "Total images in validation data is 1482\n",
            "Total images in testing data is 1482\n"
          ]
        }
      ],
      "source": [
        "# ******************** data preparation  ********************\n",
        "my_dataset = getattr(dataset, configs[\"dataset\"])\n",
        "train_data = my_dataset(data_root=data_root, mode='train')\n",
        "valid_data = my_dataset(data_root=data_root, mode='valid')\n",
        "test_data = my_dataset(data_root=data_root, mode='test')\n",
        "logger.info('Total images in training data is {}'.format(len(train_data)))\n",
        "logger.info('Total images in validation data is {}'.format(len(valid_data)))\n",
        "logger.info('Total images in testing data is {}'.format(len(test_data)))\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
        "valid_loader = DataLoader(valid_data, batch_size=8, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=8, shuffle=False)\n",
        "\n",
        "\n",
        "# ********************  ********************\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n",
        "# optimizer = optim.SGD(model.parameters(), lr = learning_rate, momentum=0.0)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, threshold=0.00001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Funciones para mostrar las métricas de rendimiento recogidas durante el entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u7c1-FI9_vKz"
      },
      "outputs": [],
      "source": [
        "def plot_loss(train_losses, valid_losses):\n",
        "    epochs = np.arange(1, len(train_losses) + 1)\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, train_losses, label='Train Loss')\n",
        "    plt.plot(epochs, valid_losses, label='Valid Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.xticks(epochs)\n",
        "    plt.show()\n",
        "\n",
        "def plot_pck(pck_values):\n",
        "    epochs = np.arange(1, len(pck_values) + 1)\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, pck_values)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('PCK')\n",
        "    plt.title('PCK Evolution')\n",
        "    plt.xticks(epochs)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparamos las variables que van a guardar los valores de las métricas a analizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Q2wDFUxu_zFP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_losses: []\n",
            "valid_losses: []\n",
            "pck_values: []\n"
          ]
        }
      ],
      "source": [
        "train_losses = []\n",
        "valid_losses = []\n",
        "pck_values = []\n",
        "with open('train_losses.pkl', 'rb') as train_losses_file:\n",
        "    train_losses = pickle.load(train_losses_file)\n",
        "with open('valid_losses.pkl', 'rb') as valid_losses_file:\n",
        "    valid_losses = pickle.load(valid_losses_file)\n",
        "with open('pck_values.pkl', 'rb') as pck_values_file:\n",
        "    pck_values = pickle.load(pck_values_file)\n",
        "\n",
        "print('train_losses:', train_losses)\n",
        "print('valid_losses:', valid_losses)\n",
        "print('pck_values:', pck_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos el entrenamiento y la evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hNpKU_vCAPPx"
      },
      "outputs": [],
      "source": [
        "def train(model):\n",
        "    logger.info('\\nStart training ===========================================>')\n",
        "    best_epo = -1\n",
        "    max_pck = -1\n",
        "    logger.info('Initial learning Rate: {}'.format(learning_rate))\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        logger.info('Epoch[{}/{}] ==============>'.format(epoch, epochs))\n",
        "        train_label_loss = []\n",
        "        model.train()\n",
        "        for step, data in enumerate(train_loader, 0):\n",
        "            # *************** target prepare ***************\n",
        "            img, label_terget, img_name, w, h = data\n",
        "            if cuda:\n",
        "                img = img.cuda(non_blocking=True)\n",
        "                label_terget = label_terget.cuda(non_blocking=True)\n",
        "            optimizer.zero_grad()\n",
        "            label_pred = model(img)\n",
        "\n",
        "            # *************** calculate loss ***************\n",
        "            label_loss = sum_mse_loss(label_pred.float(), label_terget.float())  # keypoint confidence loss\n",
        "            label_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_label_loss.append(label_loss.item())\n",
        "\n",
        "            label_loss.detach()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            if step % 10 == 0:\n",
        "                logger.info('TRAIN STEP: {}  LOSS {}'.format(step, label_loss.item()))\n",
        "\n",
        "        # *************** eval model after one epoch ***************\n",
        "        #eval_loss, cur_pck = eval(model, epoch, mode='valid') # type: ignore\n",
        "        eval_loss, cur_pck = eval(model)\n",
        "        train_losses.append(sum(train_label_loss) / len(train_label_loss))\n",
        "        valid_losses.append(eval_loss)\n",
        "        pck_values.append(cur_pck)\n",
        "        logger.info('EPOCH {} VALID PCK  {}'.format(epoch, cur_pck))\n",
        "        logger.info('EPOCH {} TRAIN_LOSS {}'.format(epoch, sum(train_label_loss) / len(train_label_loss)))\n",
        "        logger.info('EPOCH {} VALID_LOSS {}'.format(epoch, eval_loss))\n",
        "\n",
        "        # *************** save current model and best model ***************\n",
        "        if cur_pck > max_pck:\n",
        "            torch.save(model.state_dict(), os.path.join(save_dir, 'best_model.pth'))\n",
        "            torch.save(optimizer.state_dict(),  os.path.join(save_dir,'best_optimizador.pth'))\n",
        "            best_epo = epoch\n",
        "            max_pck = cur_pck\n",
        "        logger.info('Current Best EPOCH is : {}, PCK is : {}\\n**************\\n'.format(best_epo, max_pck))\n",
        "\n",
        "        # save current model\n",
        "        torch.save(model.state_dict(), os.path.join(save_dir, 'epoch_' + str(epoch) + '_' + str(cur_pck) + '.pth'))\n",
        "        torch.save(optimizer.state_dict(),  os.path.join(save_dir,'epoch_' + str(epoch) + str(cur_pck) + '_''optimizador.pth'))\n",
        "        # scheduler\n",
        "        scheduler.step(cur_pck)\n",
        "\n",
        "    logger.info('Train Done! ')\n",
        "    logger.info('Best epoch is {}'.format(best_epo))\n",
        "    logger.info('Best Valid PCK is {}'.format(max_pck))\n",
        "\n",
        "\n",
        "def eval(model, mode='valid'):\n",
        "    if mode == 'valid':\n",
        "        loader = valid_loader\n",
        "        gt_labels = valid_data.all_labels\n",
        "    else:\n",
        "        loader = test_loader\n",
        "        gt_labels = test_data.all_labels\n",
        "\n",
        "    with torch.no_grad():\n",
        "        all_pred_labels = {}  # save predict results\n",
        "        eval_loss = []\n",
        "        model.eval()\n",
        "        for step, (img, label_terget, img_name, w, h) in enumerate(loader):\n",
        "            if cuda:\n",
        "                img = img.cuda(non_blocking=True)\n",
        "            cm_pred = model(img)\n",
        "\n",
        "            all_pred_labels = get_pred_coordinates(cm_pred.cpu(), img_name, w, h, all_pred_labels)\n",
        "            loss_final = sum_mse_loss(cm_pred.cpu(), label_terget)\n",
        "            if step % 10 == 0:\n",
        "                logger.info('EVAL STEP: {}  LOSS {}'.format(step, loss_final.item()))\n",
        "            eval_loss.append(loss_final)\n",
        "\n",
        "        # ************* calculate PCKs  ************\n",
        "        pck_dict = get_pck_with_sigma(all_pred_labels, gt_labels, target_sigma_list)\n",
        "\n",
        "        select_pck = pck_dict[select_sigma]\n",
        "        eval_loss = sum(eval_loss) / len(eval_loss)\n",
        "    return eval_loss, select_pck\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ejecutamos el entrenamiento, guardamos las métricas y ejecutamos el conjunto de test con el mejor modelo obtenido tras el entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyyTCeo8_3Jw"
      },
      "outputs": [],
      "source": [
        "train(model)\n",
        "\n",
        "################## Saving metrics for evaluation ###################\n",
        "import pickle\n",
        "with open('train_losses.pkl', 'wb') as train_losses_file:\n",
        "    pickle.dump(train_losses, train_losses_file)\n",
        "with open('valid_losses.pkl', 'wb') as valid_losses_file:\n",
        "    pickle.dump(valid_losses, valid_losses_file)\n",
        "with open('pck_values.pkl', 'wb') as pck_values_file:\n",
        "    pickle.dump(pck_values, pck_values_file)\n",
        "####################################################################\n",
        "\n",
        "logger.info('\\nTESTING ============================>')\n",
        "logger.info('Load Trained model !!!')\n",
        "state_dict = torch.load(os.path.join(save_dir, 'best_model.pth'))\n",
        "model.load_state_dict(state_dict)\n",
        "eval(model, mode='test')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MqMtLmvR0yp1"
      },
      "source": [
        "Guardamos los valores de la función de pérdida y el pck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Oa7AKn2b0yAo"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('train_losses.pkl', 'wb') as train_losses_file:\n",
        "    pickle.dump(train_losses, train_losses_file)\n",
        "with open('valid_losses.pkl', 'wb') as valid_losses_file:\n",
        "    pickle.dump(valid_losses, valid_losses_file)\n",
        "with open('pck_values.pkl', 'wb') as pck_values_file:\n",
        "    pickle.dump(pck_values, pck_values_file)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Limpiar cache GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del model\n",
        "torch.cuda.empty_cache() \n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yASeHBZAa6K"
      },
      "outputs": [],
      "source": [
        "plot_loss(train_losses, valid_losses)\n",
        "plot_pck(pck_values)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uloRKf2jAAtN"
      },
      "source": [
        "# Pruebas"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BHuz1onq4Ebc"
      },
      "source": [
        "Comprobamos que la red neuronal realiza predicciones correctas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "em-096s4STth"
      },
      "outputs": [],
      "source": [
        "COLORMAP = {\n",
        "    \"thumb\": {\"ids\": [0, 1, 2, 3, 4], \"color\": \"g\"},\n",
        "    \"index\": {\"ids\": [0, 5, 6, 7, 8], \"color\": \"c\"},\n",
        "    \"middle\": {\"ids\": [0, 9, 10, 11, 12], \"color\": \"b\"},\n",
        "    \"ring\": {\"ids\": [0, 13, 14, 15, 16], \"color\": \"m\"},\n",
        "    \"little\": {\"ids\": [0, 17, 18, 19, 20], \"color\": \"r\"},\n",
        "}\n",
        "test_data = dataset.HandDataset(data_root=data_root, mode='test')\n",
        "test_loader = DataLoader(test_data, batch_size=5, shuffle=False)\n",
        "img, cm_target, img_name, w, h= next(iter(test_loader))\n",
        "    # ***************** draw Limb map *****************\n",
        "def plot_hand(hand_points, colormap, image):\n",
        "    # Create the hand plot\n",
        "    fig, ax = plt.subplots()\n",
        "    for finger in colormap:\n",
        "        # Get the keypoint IDs and color for each finger\n",
        "        ids = colormap[finger]['ids']\n",
        "        color = colormap[finger]['color']\n",
        "        # Get the x and y coordinates for each keypoint\n",
        "        x = [hand_points[i][0] for i in ids]\n",
        "        y = [hand_points[i][1] for i in ids]\n",
        "        # Draw lines between the keypoints for each finger\n",
        "        ax.plot(x, y, color=color, linewidth=2)\n",
        "        # Draw each keypoint with the corresponding finger color and a black border\n",
        "        ax.scatter(x, y, s=50, color=color, edgecolor='k')\n",
        "\n",
        "    # Overlay the plot onto the image\n",
        "    ax.imshow(image)\n",
        "    ax.set_aspect('equal', 'box')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  all_pred_labels = {}\n",
        "  all_target_labels = {}\n",
        "  model.eval()\n",
        "  if cuda:\n",
        "    img = img.cuda()\n",
        "  cm_pred = model(img)\n",
        "  dictionary_target= get_pred_coordinates(cm_target.cpu(),img_name, w, h, all_target_labels)\n",
        "  dictionary_pred=get_pred_coordinates(cm_pred.cpu(),img_name, w, h, all_pred_labels)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qUxKUUQp5fi6"
      },
      "source": [
        "### Etiquetas reales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OwEgvUT5c8v"
      },
      "outputs": [],
      "source": [
        "for i,values in enumerate(dictionary_target.values()):\n",
        "  image = Image.open(os.path.join(data_root, 'imgs', img_name[i]))\n",
        "  true_keypoints_img = list(values.values())[0]\n",
        "  plot_hand(true_keypoints_img, COLORMAP, image)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HK1wLKsq5mY1"
      },
      "source": [
        "### Etiquetas predecidas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJJOS4KV5qMZ"
      },
      "outputs": [],
      "source": [
        "for i,values in enumerate(dictionary_pred.values()):\n",
        "  image = Image.open(os.path.join(data_root, 'imgs', img_name[i]))\n",
        "  pred_keypoints_img = list(values.values())[0]\n",
        "  plot_hand(pred_keypoints_img, COLORMAP, image)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
